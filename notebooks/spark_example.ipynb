{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark with Docker Example\n",
    "\n",
    "This notebook demonstrates how to connect to a Spark cluster running in Docker and perform basic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created successfully!\n",
      "Spark Version: 3.5.0\n",
      "Spark Master: spark://spark-master:7077\n",
      "Spark App Name: Docker Spark Example\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session with proper configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Docker Spark Example\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark Session created successfully!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")\n",
    "print(f\"Spark App Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory exists: /home/jovyan/data\n",
      "Contents of /home/jovyan/data: ['sample_data.parquet']\n"
     ]
    }
   ],
   "source": [
    "# Check data directory exists and create if needed\n",
    "data_dir = \"/home/jovyan/data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    print(f\"Created data directory: {data_dir}\")\n",
    "else:\n",
    "    print(f\"Data directory exists: {data_dir}\")\n",
    "\n",
    "# List contents of data directory\n",
    "print(f\"Contents of {data_dir}: {os.listdir(data_dir) if os.path.exists(data_dir) else 'Directory not found'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing basic Spark RDD operations...\n",
      "RDD map result: [2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# Test basic Spark functionality\n",
    "print(\"Testing basic Spark RDD operations...\")\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "result = rdd.map(lambda x: x * 2).collect()\n",
    "print(f\"RDD map result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataFrame...\n",
      "DataFrame created successfully!\n",
      "+-------+---+--------------+\n",
      "|   Name|Age|           Job|\n",
      "+-------+---+--------------+\n",
      "|  Alice| 25|      Engineer|\n",
      "|    Bob| 30|Data Scientist|\n",
      "|Charlie| 35|       Manager|\n",
      "|  Diana| 28|       Analyst|\n",
      "|    Eve| 32|     Developer|\n",
      "+-------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a simple DataFrame\n",
    "print(\"Creating DataFrame...\")\n",
    "data = [(\"Alice\", 25, \"Engineer\"),\n",
    "        (\"Bob\", 30, \"Data Scientist\"),\n",
    "        (\"Charlie\", 35, \"Manager\"),\n",
    "        (\"Diana\", 28, \"Analyst\"),\n",
    "        (\"Eve\", 32, \"Developer\")]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Job\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "print(\"DataFrame created successfully!\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema:\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Job: string (nullable = true)\n",
      "\n",
      "\n",
      "DataFrame Count: 5\n",
      "\n",
      "Filtering data (Age > 30):\n",
      "+-------+---+---------+\n",
      "|   Name|Age|      Job|\n",
      "+-------+---+---------+\n",
      "|Charlie| 35|  Manager|\n",
      "|    Eve| 32|Developer|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform some basic operations\n",
    "print(\"DataFrame Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(f\"\\nDataFrame Count: {df.count()}\")\n",
    "\n",
    "print(\"\\nFiltering data (Age > 30):\")\n",
    "df.filter(col(\"Age\") > 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average age by job:\n",
      "+--------------+-----------+\n",
      "|           Job|Average_Age|\n",
      "+--------------+-----------+\n",
      "|Data Scientist|       30.0|\n",
      "|      Engineer|       25.0|\n",
      "|     Developer|       32.0|\n",
      "|       Analyst|       28.0|\n",
      "|       Manager|       35.0|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by and aggregate\n",
    "print(\"Average age by job:\")\n",
    "df.groupBy(\"Job\").agg(avg(\"Age\").alias(\"Average_Age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating larger dataset...\n",
      "Large dataset created with 1000 rows\n",
      "+--------+---+--------------+------+\n",
      "|    Name|Age|           Job|Salary|\n",
      "+--------+---+--------------+------+\n",
      "|Person_0| 44|      Engineer|135187|\n",
      "|Person_1| 29|Data Scientist|122086|\n",
      "|Person_2| 52|      Designer| 65222|\n",
      "|Person_3| 59|       Manager| 60684|\n",
      "|Person_4| 49|       Manager| 50889|\n",
      "|Person_5| 37|     Developer| 65507|\n",
      "|Person_6| 33|       Manager|142709|\n",
      "|Person_7| 65|     Developer|115918|\n",
      "|Person_8| 29|Data Scientist|104199|\n",
      "|Person_9| 60|       Analyst|120133|\n",
      "+--------+---+--------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a larger dataset for demonstration\n",
    "import random\n",
    "\n",
    "print(\"Creating larger dataset...\")\n",
    "# Create sample data\n",
    "large_data = []\n",
    "jobs = [\"Engineer\", \"Data Scientist\", \"Manager\", \"Analyst\", \"Developer\", \"Designer\"]\n",
    "\n",
    "for i in range(1000):\n",
    "    name = f\"Person_{i}\"\n",
    "    age = random.randint(22, 65)\n",
    "    job = random.choice(jobs)\n",
    "    salary = random.randint(50000, 150000)\n",
    "    large_data.append((name, age, job, salary))\n",
    "\n",
    "large_schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Job\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "large_df = spark.createDataFrame(large_data, large_schema)\n",
    "print(f\"Large dataset created with {large_df.count()} rows\")\n",
    "large_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics by Job:\n",
      "+--------------+-----+------------------+------------------+----------+----------+\n",
      "|           Job|Count|           Avg_Age|        Avg_Salary|Max_Salary|Min_Salary|\n",
      "+--------------+-----+------------------+------------------+----------+----------+\n",
      "|       Manager|  187| 43.07486631016043| 101971.2192513369|    149800|     50217|\n",
      "|      Designer|  166| 44.04216867469879|101986.78313253012|    149920|     50060|\n",
      "|     Developer|  164|43.390243902439025| 98738.24390243902|    149860|     50028|\n",
      "|Data Scientist|  162|42.370370370370374| 97090.63580246913|    149957|     50091|\n",
      "|      Engineer|  162| 43.50617283950617|101106.51851851853|    149921|     50587|\n",
      "|       Analyst|  159|42.710691823899374| 102314.2075471698|    149951|     51731|\n",
      "+--------------+-----+------------------+------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform analytics on the larger dataset\n",
    "print(\"Statistics by Job:\")\n",
    "stats_df = large_df.groupBy(\"Job\").agg(\n",
    "    count(\"*\").alias(\"Count\"),\n",
    "    avg(\"Age\").alias(\"Avg_Age\"),\n",
    "    avg(\"Salary\").alias(\"Avg_Salary\"),\n",
    "    max(\"Salary\").alias(\"Max_Salary\"),\n",
    "    min(\"Salary\").alias(\"Min_Salary\")\n",
    ").orderBy(\"Count\", ascending=False)\n",
    "\n",
    "stats_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting small DataFrame to Pandas:\n",
      "      Name  Age             Job\n",
      "0    Alice   25        Engineer\n",
      "1      Bob   30  Data Scientist\n",
      "2  Charlie   35         Manager\n",
      "3    Diana   28         Analyst\n",
      "4      Eve   32       Developer\n",
      "\n",
      "Pandas DataFrame type: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Convert to Pandas for visualization (small dataset)\n",
    "print(\"Converting small DataFrame to Pandas:\")\n",
    "pandas_df = df.toPandas()\n",
    "print(pandas_df)\n",
    "print(f\"\\nPandas DataFrame type: {type(pandas_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data as Parquet file...\n",
      "Data saved successfully!\n",
      "Reading Parquet file back...\n",
      "Read 1000 rows from Parquet file\n",
      "First few rows from saved file:\n",
      "+--------+---+--------------+------+\n",
      "|    Name|Age|           Job|Salary|\n",
      "+--------+---+--------------+------+\n",
      "|Person_0| 44|      Engineer|135187|\n",
      "|Person_1| 29|Data Scientist|122086|\n",
      "|Person_2| 52|      Designer| 65222|\n",
      "|Person_3| 59|       Manager| 60684|\n",
      "|Person_4| 49|       Manager| 50889|\n",
      "+--------+---+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save DataFrame as Parquet with error handling\n",
    "try:\n",
    "    print(\"Saving data as Parquet file...\")\n",
    "    \n",
    "    # Use a path that's accessible to all containers\n",
    "    parquet_path = \"/home/jovyan/data/sample_data.parquet\"\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(parquet_path), exist_ok=True)\n",
    "    \n",
    "    # Coalesce to single partition for easier file handling\n",
    "    large_df.coalesce(1).write.mode(\"overwrite\").parquet(parquet_path)\n",
    "    print(\"Data saved successfully!\")\n",
    "    \n",
    "    # Read it back to verify\n",
    "    print(\"Reading Parquet file back...\")\n",
    "    read_df = spark.read.parquet(parquet_path)\n",
    "    print(f\"Read {read_df.count()} rows from Parquet file\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(\"First few rows from saved file:\")\n",
    "    read_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error saving/reading Parquet file: {e}\")\n",
    "    print(\"Trying alternative approach - saving as CSV instead...\")\n",
    "    \n",
    "    try:\n",
    "        csv_path = \"/home/jovyan/data/sample_data.csv\"\n",
    "        large_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_path)\n",
    "        print(\"Data saved as CSV successfully!\")\n",
    "        \n",
    "        # Read CSV back\n",
    "        read_csv_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_path)\n",
    "        print(f\"Read {read_csv_df.count()} rows from CSV file\")\n",
    "        \n",
    "    except Exception as csv_error:\n",
    "        print(f\"Error with CSV: {csv_error}\")\n",
    "        print(\"File operations may require additional configuration for this environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Spark Session Summary ===\n",
      "Spark UI: http://localhost:8080\n",
      "Application ID: app-20250601151341-0000\n",
      "Default Parallelism: 2\n",
      "Spark Version: 3.5.0\n",
      "\n",
      "Data files created: ['sample_data.parquet']\n"
     ]
    }
   ],
   "source": [
    "# Show Spark UI URL and execution summary\n",
    "print(\"\\n=== Spark Session Summary ===\")\n",
    "print(f\"Spark UI: http://localhost:8080\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "\n",
    "# Check if files were created\n",
    "import os\n",
    "data_files = [f for f in os.listdir(\"/home/jovyan/data\") if f.endswith(('.parquet', '.csv', '.json'))]\n",
    "print(f\"\\nData files created: {data_files if data_files else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped successfully!\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
